Use existing infrastructure via data sources
✔ Create:

EKS Cluster

Fargate Profiles

IAM roles

ECR repositories

ArgoCD deployment (Helm/K8s)

Kubernetes namespaces

CI/CD support resources

Suggested Terraform Infra

terraform/
│
├── backend.tf
├── providers.tf
├── variables.tf
├── main.tf
├── outputs.tf
│
├── env/
│   ├── dev.tfvars
│   ├── staging.tfvars
│   └── prod.tfvars
│
├── modules/
│   ├── eks/
│   ├── fargate/
│   ├── iam/
│   ├── irsa/
│   ├── autoscaler/
│   └── external-secrets/

-------------------------------------------
backend.tf with s3+dynamodb locking

terraform {
  backend "s3" {
    bucket         = "my-terraform-state-bucket"
    key            = "eks/${terraform.workspace}/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}
-----------------------------------------
providers.tf

pprovider "aws" {
  region = var.aws_region
}

data "aws_eks_cluster_auth" "this" {
  name = module.eks.cluster_name
}

provider "kubernetes" {
  host                   = module.eks.cluster_endpoint
  token                  = data.aws_eks_cluster_auth.this.token
  cluster_ca_certificate = base64decode(module.eks.cluster_ca)
}

provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    token                  = data.aws_eks_cluster_auth.this.token
    cluster_ca_certificate = base64decode(module.eks.cluster_ca)
  }
}
------------------------------------------------
variables.tf

variable "aws_region" {}
variable "cluster_name" {}
variable "vpc_id" {}
variable "private_subnet_ids" {
  type = list(string)
}
variable "public_subnet_ids" {
  type = list(string)
}
-------------------------------------------------
env/dev.tfvars

aws_region = "us-east-1"
cluster_name = "myapp-dev"
vpc_id = "vpc-dev"
private_subnet_ids = ["subnet-dev1", "subnet-dev2"]
public_sunnet_ids  = ["subnet-dev1", "subnet-dev2"]
---------------
env/staging.tfvars

aws_region = "us-east-1"
cluster_name = "myapp-staging"
vpc_id = "vpc-stg"
private_subnet_ids = ["subnet-stg1", "subnet-stg2"]
-------------
env/prod.tfvars

aws_region = "us-east-1"
cluster_name = "myapp-prod"
vpc_id = "vpc-prod"
private_subnet_ids = ["subnet-prod1", "subnet-prod2"]

----------------------------------------------------
Root Main
main.tf

module "iam" {
  source = "./modules/iam"
  cluster_name = var.cluster_name
}

module "eks" {
  source = "./modules/eks"

  cluster_name      = var.cluster_name
  subnet_ids        = var.private_subnet_ids
  cluster_role_arn  = module.iam.eks_cluster_role_arn
}

module "fargate" {
  source = "./modules/fargate"

  cluster_name = module.eks.cluster_name
  subnet_ids   = var.private_subnet_ids
  pod_execution_role_arn = module.iam.fargate_pod_role_arn
}

module "irsa_autoscaler" {
  source = "./modules/irsa"

  cluster_name = module.eks.cluster_name
  oidc_provider_arn = module.eks.oidc_provider_arn
  namespace = "kube-system"
  service_account = "cluster-autoscaler"
}

module "autoscaler" {
  source = "./modules/autoscaler"

  cluster_name = module.eks.cluster_name
}

module "irsa_external_secrets" {
  source = "./modules/irsa"

  cluster_name = module.eks.cluster_name
  oidc_provider_arn = module.eks.oidc_provider_arn
  namespace = "external-secrets"
  service_account = "external-secrets"
}

module "external_secrets" {
  source = "./modules/external-secrets"
}
----------------------------------------

----------------------------------------
outputs.tf

output "cluster_name" {
  value = module.eks.cluster_name
}

output "cluster_endpoint" {
  value = module.eks.cluster_endpoint
}
--------------------------------------------
MODULES
--------------------------------------------
modules/iam/main.tf

resource "aws_iam_role" "eks_cluster_role" {
  name = "${var.cluster_name}-eks-role"

  assume_role_policy = data.aws_iam_policy_document.eks_assume_role.json
}

data "aws_iam_policy_document" "eks_assume_role" {
  statement {
    actions = ["sts:AssumeRole"]

    principals {
      type = "Service"
      identifiers = ["eks.amazonaws.com"]
    }
  }
}

resource "aws_iam_role_policy_attachment" "eks_policy" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role" "fargate_pod_role" {
  name = "${var.cluster_name}-fargate-role"

  assume_role_policy = data.aws_iam_policy_document.fargate_assume_role.json
}

data "aws_iam_policy_document" "fargate_assume_role" {
  statement {
    actions = ["sts:AssumeRole"]

    principals {
      type = "Service"
      identifiers = ["eks-fargate-pods.amazonaws.com"]
    }
  }
}
---------------------------------------------------------------
modules/iam/policies.tf
cluster auto-scaler policy

resource "aws_iam_policy" "cluster_autoscaler" {
  name = "${var.cluster_name}-autoscaler"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "autoscaling:*",
          "ec2:Describe*",
          "eks:DescribeNodegroup"
        ]
        Resource = "*"
      }
    ]
  })
}
----------------------------------------------------------------
modules/iam/variables.tf

variable "cluster_name" {}
---------------------------------------------------------
modules/iam/outputs.tf

output "eks_cluster_role_arn" {
  value = aws_iam_role.eks_cluster_role.arn
}

output "fargate_pod_role_arn" {
  value = aws_iam_role.fargate_pod_role.arn
}
----------------------------------------------------------
modules/eks/main.tf

resource "aws_eks_cluster" "this" {
  name     = var.cluster_name
  role_arn = var.cluster_role_arn

  vpc_config {
    subnet_ids = var.subnet_ids
  }
}

data "aws_eks_cluster" "this" {
  name = aws_eks_cluster.this.name
}

resource "aws_iam_openid_connect_provider" "this" {
  url = data.aws_eks_cluster.this.identity[0].oidc[0].issuer

  client_id_list = ["sts.amazonaws.com"]

  thumbprint_list = ["9e99a48a9960b14926bb7f3b02e22da0ecd4e"]
}
--------------------------------------------------------
modules/eks/variables.tf

variable "cluster_name" {}
variable "cluster_role_arn" {}
variable "subnet_ids" {
  type = list(string)
}
variable "vpc_id" {}
-------------------------------------------------------
modules/eks/outputs.tf

output "cluster_name" {
  value = aws_eks_cluster.this.name
}

output "cluster_endpoint" {
  value = aws_eks_cluster.this.endpoint
}

output "cluster_ca" {
  value = aws_eks_cluster.this.certificate_authority[0].data
}

output "oidc_provider_arn" {
  value = aws_iam_openid_connect_provider.this.arn
}
-------------------------------------------------------
modules/irsa/main.tf

data "aws_iam_policy_document" "assume_role" {
  statement {
    actions = ["sts:AssumeRoleWithWebIdentity"]

    principals {
      type        = "Federated"
      identifiers = [var.oidc_provider_arn]
    }

    condition {
      test     = "StringEquals"
      variable = "${replace(var.oidc_provider_arn, "arn:aws:iam::", "")}:sub"
      values   = ["system:serviceaccount:${var.namespace}:${var.service_account}"]
    }
  }
}

resource "aws_iam_role" "this" {
  name = "${var.cluster_name}-${var.service_account}"
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}
-------------------------------------------------------
modules/irsa/policy-attachments.tf

resource "aws_iam_role_policy_attachment" "this" {
  role       = aws_iam_role.this.name
  policy_arn = var.policy_arn
}
-------------------------------------------------------
modules/irsa/variables.tf

variable "cluster_name" {}
variable "oidc_provider_arn" {}
variable "namespace" {}
variable "service_account" {}
variable "policy_arn" {}
-------------------------------------------------------
modules/autoscaler/main.tf

resource "helm_release" "autoscaler" {
  name       = "cluster-autoscaler"
  namespace  = "kube-system"
  repository = "https://kubernetes.github.io/autoscaler"
  chart      = "cluster-autoscaler"

  set {
    name  = "autoDiscovery.clusterName"
    value = var.cluster_name
  }

  set {
    name  = "awsRegion"
    value = var.aws_region
  }
}
-------------------------------------------------------
modules/autoscaler/variables.tf

variable "cluster_name" {}
variable "aws_region" {
  default = "us-east-1"
}
-------------------------------------------------------
modules/external-secrets/main.tf

resource "kubernetes_namespace" "this" {
  metadata {
    name = "external-secrets"
  }
}

resource "helm_release" "external_secrets" {
  name       = "external-secrets"
  namespace  = "external-secrets"
  repository = "https://charts.external-secrets.io"
  chart      = "external-secrets"
}
-------------------------------------------------------
modules/external-secrets/policy.tf

resource "aws_iam_policy" "external_secrets" {
  name = "${var.cluster_name}-external-secrets"

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "secretsmanager:GetSecretValue",
          "secretsmanager:DescribeSecret"
        ]
        Resource = "*"
      }
    ]
  })
}
-------------------------------------------------------
modules/aws-loadbalancer/main.tf

resource "helm_release" "alb_controller" {
  name       = "aws-load-balancer-controller"
  namespace  = "kube-system"
  repository = "https://aws.github.io/eks-charts"
  chart      = "aws-load-balancer-controller"

  set {
    name  = "clusterName"
    value = var.cluster_name
  }
}
-------------------------------------------------------
modules/karpenter/main.tf

resource "helm_release" "karpenter" {
  name       = "karpenter"
  namespace  = "karpenter"
  repository = "https://charts.karpenter.sh"
  chart      = "karpenter"

  set {
    name  = "settings.clusterName"
    value = var.cluster_name
  }
}
-------------------------------------------------------
modules/external-secrets/outputs.tf

output "cluster_name" {
  value = module.eks.cluster_name
}
-------------------------------------------------------
modules/fargate/main.tf

resource "aws_eks_fargate_profile" "default" {
  cluster_name = var.cluster_name
  pod_execution_role_arn = var.pod_execution_role_arn
  subnet_ids = var.subnet_ids

  selector {
    namespace = "default"
  }
}
--------------------------------------------------------
modules/ecr/main.tf

resource "aws_ecr_repository" "frontend" {
  name = "frontend"
}

resource "aws_ecr_repository" "backend" {
  name = "backend"
}

resource "aws_ecr_repository" "mobile" {
  name = "mobile"
}
--------------------------------------------------------
modules/argocd/main.tf #Deploy via helm

resource "kubernetes_namespace" "argocd" {
  metadata {
    name = "argocd"
  }
}

resource "helm_release" "argocd" {
  name       = "argocd"
  namespace  = "argocd"
  repository = "https://argoproj.github.io/argo-helm"
  chart      = "argo-cd"
}
--------------------------------------------------------
resource "kubernetes_namespace" "karpenter" {
  metadata {
    name = "karpenter"
  }
}
--------------------------------------------------------
Dynamodb locak table

resource "aws_dynamodb_table" "terraform_locks" {
  name         = "terraform-locks"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}
----------------------------------------------------------
.gitlab-ci.yaml    #gitlab cicd example

stages:
  - build
  - scan
  - push
  - update-gitops

variables:
  IMAGE_TAG: $CI_COMMIT_SHORT_SHA
  AWS_REGION: us-east-1

build:
  stage: build
  script:
    - docker build -t $CI_PROJECT_NAME:$IMAGE_TAG .

sonarqube:
  stage: scan
  script:
    - sonar-scanner

trivy:
  stage: scan
  script:
    - trivy image $CI_PROJECT_NAME:$IMAGE_TAG

push-ecr:
  stage: push
  script:
    - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin <acct>.dkr.ecr.$AWS_REGION.amazonaws.com
    - docker tag $CI_PROJECT_NAME:$IMAGE_TAG <acct>.dkr.ecr.$AWS_REGION.amazonaws.com/$CI_PROJECT_NAME:$IMAGE_TAG
    - docker push <acct>.dkr.ecr.$AWS_REGION.amazonaws.com/$CI_PROJECT_NAME:$IMAGE_TAG

update-gitops:
  stage: update-gitops
  script:
    - sed -i "s/tag:.*/tag: $IMAGE_TAG/" charts/frontend/values.yaml
    - git commit -am "Update image"
    - git push
---------------------------------------------------------------------
external-secret.yaml

apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: db-secret
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: aws-secretsmanager
    kind: SecretStore
  target:
    name: db-secret
  data:
    - secretKey: password
      remoteRef:
        key: prod/db/password
---------------------------------------------------------------------
charts/frontend/Chart.yaml

apiVersion: v2
name: frontend
version: 0.1.0
----------------------------
values.yaml

image:
  repository: <acct>.dkr.ecr.us-east-1.amazonaws.com/frontend
  tag: latest
-----------------------------
templates/deployments.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: frontend
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          ports:
            - containerPort: 80
---------------------------------------------------------------------


---------------------------------------------------------------------
GitOps Flow

GitLab Push →
Build Docker →
SonarQube Scan →
Trivy Scan →
Push to ECR →
Update Helm/K8s manifests →
ArgoCD Sync →
Deploy to EKS Fargate
----------------------------------------------------------------------
Workspace Usage
-->Crate Workspaces

terraform init

terraform workspace new dev
terraform workspace new staging
terraform workspace new prod

--> Deploy DEV
terraform workspace select dev
terraform apply -var-file=env/dev.tfvars

-->Deploy STAGING
terraform workspace select staging
terraform apply -var-file=env/staging.tfvars

-->Deploy PROD
terraform workspace select prod
terraform apply -var-file=env/prod.tfvars

For 1 cluster + 18–25 services, best practice:
Hybrid Model → Fargate + EC2 nodes (Karpenter-managed)
Why NOT Fargate-only?
Fargate-only limitations:
No DaemonSets (breaks many agents)
Higher cost at scale
Limited control over node sizing
Cluster Autoscaler unnecessary
Some controllers need EC2 (ALB, CSI, monitoring)

Why Hybrid is Industry Standard
Run system workloads on EC2 nodes
ALB Controller
Karpenter
Autoscaler / metrics / logging / security agents
Run applications on Fargate
frontend / backend / APIs / microservices
Cost optimized
More flexible scaling
Supports full Kubernetes ecosystem

Final Architecture

EKS Cluster
│
├── Fargate Profiles → App workloads
├── EC2 Nodes → System workloads
│
├── IRSA
├── Karpenter (node provisioning)
├── AWS Load Balancer Controller
├── External Secrets
├── ArgoCD (GitOps)

-----------------------------------------------------------------------
ArgoCD GitOps Applications

argocd/apps/frontend.yaml  (same patter for backend/mobile/services)

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: frontend
spec:
  project: default
  source:
    repoURL: https://gitlab.com/myorg/gitops-repo.git
    targetRevision: HEAD
    path: charts/frontend
  destination:
    server: https://kubernetes.default.svc
    namespace: frontend
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----------------------------------------------------------
external-secret.yaml











